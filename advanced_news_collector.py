import feedparser
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import time
import re

class NewsCollector:
    def __init__(self):
        self.articles = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def extract_media_name(self, url):
        """URLã‹ã‚‰ãƒ¡ãƒ‡ã‚£ã‚¢åã‚’æŠ½å‡º"""
        domain = urlparse(url).netloc.lower()
        if 'cnn.com' in domain:
            return 'CNN'
        elif 'foxnews.com' in domain:
            return 'Fox News'
        elif 'nytimes.com' in domain:
            return 'New York Times'
        elif 'wsj.com' in domain:
            return 'Wall Street Journal'
        elif 'breitbart.com' in domain:
            return 'Breitbart'
        elif 'bbc.com' in domain or 'bbc.co.uk' in domain:
            return 'BBC'
        elif 'theguardian.com' in domain:
            return 'The Guardian'
        elif 'telegraph.co.uk' in domain:
            return 'Telegraph'
        elif 'reuters.com' in domain:
            return 'Reuters'
        else:
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰æ¨æ¸¬
            return domain.replace('www.', '').split('.')[0].title()

    def collect_from_rss(self, rss_url, max_articles=5):
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹åé›†"""
        try:
            print(f"RSSåé›†ä¸­: {rss_url}")
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                print(f"âš ï¸ RSSè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {rss_url}")
                return []
            
            articles = []
            for entry in feed.entries[:max_articles]:
                article = {
                    'title': entry.title,
                    'url': entry.link,
                    'published': entry.published if 'published' in entry else 'N/A',
                    'summary': entry.summary if 'summary' in entry else '',
                    'media': self.extract_media_name(entry.link),
                    'collection_method': 'RSS',
                    'collected_at': datetime.now().isoformat()
                }
                articles.append(article)
            
            print(f"âœ… RSS: {len(articles)}ä»¶åé›†å®Œäº†")
            return articles
            
        except Exception as e:
            print(f"âŒ RSSã‚¨ãƒ©ãƒ¼ {rss_url}: {str(e)}")
            return []

    def collect_from_html(self, url, selectors, max_articles=5):
        """HTMLãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹åé›†"""
        try:
            print(f"HTMLåé›†ä¸­: {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []
            
            # è¦‹å‡ºã—ã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
            headlines = soup.select(selectors['headlines'])[:max_articles]
            
            for headline in headlines:
                # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                link = headline.get('href') or headline.find('a')
                if link:
                    if hasattr(link, 'get'):
                        article_url = link.get('href')
                    else:
                        article_url = link
                    
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if article_url and not article_url.startswith('http'):
                        article_url = urljoin(url, article_url)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title = headline.get_text(strip=True)
                    if title and article_url:
                        article = {
                            'title': title,
                            'url': article_url,
                            'published': 'N/A',
                            'summary': '',
                            'media': self.extract_media_name(url),
                            'collection_method': 'HTML',
                            'collected_at': datetime.now().isoformat()
                        }
                        articles.append(article)
            
            print(f"âœ… HTML: {len(articles)}ä»¶åé›†å®Œäº†")
            return articles
            
        except Exception as e:
            print(f"âŒ HTMLã‚¨ãƒ©ãƒ¼ {url}: {str(e)}")
            return []

    def collect_all_news(self):
        """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        # RSSå¯¾å¿œã‚µã‚¤ãƒˆ
        rss_sources = [
            'https://rss.cnn.com/rss/edition.rss',
            'https://moxie.foxnews.com/google-publisher/latest.xml',
            'https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml',
            'https://feeds.content.dowjones.io/public/rss/RSSUSnews',
            'https://feeds.feedburner.com/breitbart',
            'https://feeds.bbci.co.uk/news/rss.xml',
            'https://www.theguardian.com/world/rss',
        ]
        
        # HTMLæŠ½å‡ºã‚µã‚¤ãƒˆï¼ˆRSSãŒãªã„/ä¸å®‰å®šãªå ´åˆï¼‰
        html_sources = [
            {
                'url': 'https://www.telegraph.co.uk/',
                'selectors': {'headlines': 'h3 a, h2 a'}
            },
            {
                'url': 'https://www.spectator.co.uk/',
                'selectors': {'headlines': 'h3 a, h2 a'}
            }
        ]
        
        all_articles = []
        
        # RSSåé›†
        for rss_url in rss_sources:
            articles = self.collect_from_rss(rss_url, max_articles=3)
            all_articles.extend(articles)
            time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        # HTMLåé›†
        for html_source in html_sources:
            articles = self.collect_from_html(
                html_source['url'], 
                html_source['selectors'], 
                max_articles=3
            )
            all_articles.extend(articles)
            time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        # é‡è¤‡é™¤å»
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        self.articles = unique_articles
        print(f"ğŸ‰ åé›†å®Œäº†: {len(unique_articles)}ä»¶ã®è¨˜äº‹")
        
        return unique_articles

    def save_results(self):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_data = {
            'collection_time': datetime.now().isoformat(),
            'total_articles': len(self.articles),
            'sources_summary': self._get_sources_summary(),
            'articles': self.articles
        }
        
        with open('news_output.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: news_output.json")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\nğŸ“Š åé›†ã‚µãƒãƒªãƒ¼:")
        for media, count in self._get_sources_summary().items():
            print(f"  {media}: {count}ä»¶")

    def _get_sources_summary(self):
        """ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥è¨˜äº‹æ•°ã®ã‚µãƒãƒªãƒ¼"""
        summary = {}
        for article in self.articles:
            media = article['media']
            summary[media] = summary.get(media, 0) + 1
        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))

def main():
    collector = NewsCollector()
    collector.collect_all_news()
    collector.save_results()

if __name__ == "__main__":
    main()
